{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve,auc\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dianu\\anaconda3\\envs\\torcher\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = load_dataset('emotion')\n",
    "\n",
    "emotions.set_format('pandas')\n",
    "df = emotions['train'][:]\n",
    "df.head()\n",
    "\n",
    "def label_int2str(row):\n",
    "    return emotions['train'].features['label'].int2str(row)\n",
    "\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader,device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in data.items()}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, axis=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(data['labels'].cpu().numpy())\n",
    "\n",
    "    predictions, true_labels = np.array(predictions), np.array(true_labels)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dianu\\anaconda3\\envs\\torcher\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anger  -----------------------------  [1. 0. 0. 0. 0. 0.]\n",
      " fear  -----------------------------  [0. 1. 0. 0. 0. 0.]\n",
      " joy  -----------------------------  [0. 0. 1. 0. 0. 0.]\n",
      " love  -----------------------------  [0. 0. 0. 1. 0. 0.]\n",
      " sadness  -----------------------------  [0. 0. 0. 0. 1. 0.]\n",
      " surprise  -----------------------------  [0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_list_p=df[\"text\"].tolist()\n",
    "labels_p=df[\"label_name\"].tolist()\n",
    "\n",
    "text_list=[]\n",
    "labels=[]\n",
    "for i in range(len(labels_p)):\n",
    "    if text_list_p[i]!='':\n",
    "        text_list.append(text_list_p[i])\n",
    "        labels.append(labels_p[i])\n",
    "\n",
    "labels=np.array(labels)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "num_classes=int(np.max(integer_encoded)+1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "mapping = dict(zip(label_encoder.classes_, onehot_encoder.transform(label_encoder.transform(label_encoder.classes_).reshape(num_classes, 1))))\n",
    "for key,value in mapping.items():\n",
    "    print(\"\",key,\" ----------------------------- \",value)\n",
    "\n",
    "\n",
    "text_listp= [str(text) for text in text_list]\n",
    "text_list=text_listp\n",
    "\n",
    "# randomly create indices for train and test\n",
    "indices = np.arange(len(text_list))\n",
    "np.random.shuffle(indices)\n",
    "splitter=0.6\n",
    "train_indices = indices[:int(splitter*len(text_list))]\n",
    "test_indices = indices[int(splitter*len(text_list)):]\n",
    "text_list_train = np.array(text_list)[train_indices]\n",
    "text_list_test = np.array(text_list)[test_indices]\n",
    "labels_train = np.array(labels)[train_indices]\n",
    "labels_test = np.array(labels)[test_indices]\n",
    "onehot_encoded_train = np.array(onehot_encoded)[train_indices]\n",
    "onehot_encoded_test = np.array(onehot_encoded)[test_indices]\n",
    "integer_encoded_train = np.array(integer_encoded)[train_indices]\n",
    "integer_encoded_test = np.array(integer_encoded)[test_indices]\n",
    "\n",
    "text_list_train=text_list_train.tolist()\n",
    "text_list_test=text_list_test.tolist()\n",
    "labels_train=labels_train.tolist()\n",
    "labels_test=labels_test.tolist()\n",
    "onehot_encoded_train=onehot_encoded_train.tolist()\n",
    "onehot_encoded_test=onehot_encoded_test.tolist()\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(text_list_train, integer_encoded_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataset = TextDataset(text_list_test, integer_encoded_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\dianu\\anaconda3\\envs\\torcher\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.4800435796193778\n",
      "Validation Metrics: Accuracy: 0.92875, F1: 0.9283317845033622, Precision: 0.9285011337164571, Recall: 0.92875\n",
      "-------------------------------------------------------------------------\n",
      "Epoch: 1, Loss: 0.15828492659066493\n",
      "Validation Metrics: Accuracy: 0.9209375, F1: 0.9197349447679253, Precision: 0.9210890995911762, Recall: 0.9209375\n",
      "-------------------------------------------------------------------------\n",
      "Epoch: 2, Loss: 0.11085968179162592\n",
      "Validation Metrics: Accuracy: 0.92828125, F1: 0.928589429983056, Precision: 0.9300843902463825, Recall: 0.92828125\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(labels)))\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    run_loss = 0.0\n",
    "    for _, data in enumerate(train_dataloader):\n",
    "        inputs = {k: v.to(device) for k, v in data.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        run_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {run_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    train(epoch)\n",
    "    accuracy, f1, precision, recall = evaluate(model, test_dataloader, device)\n",
    "    print(f\"Validation Metrics: Accuracy: {accuracy}, F1: {f1}, Precision: {precision}, Recall: {recall}\")\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorfloid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
